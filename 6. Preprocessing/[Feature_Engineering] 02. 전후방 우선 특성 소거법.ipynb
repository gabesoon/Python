{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 선택 방법\n",
    "---\n",
    "\n",
    "- 발굴 or 생성한 feature중 의미 있는 feature를 선별해야 한다.\n",
    "- 선별 방식은 Traget과의 관련성을 평가한다.\n",
    "- 기본적으로 Filtering method 와 Wrapper method 2가지 방법이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering method\n",
    "---\n",
    "\n",
    "![그림1](https://user-images.githubusercontent.com/74717033/135281814-42257c19-2abd-4e6d-bba5-a07aada1525c.png)\n",
    "\n",
    "\n",
    "- 모든 feature가 있을때 target과 feature간의 통계적 관계성을 검정하는 과정\n",
    "    - by [ 카이제곱 검정, t-value, f-score, feature variance(변화), 상호 정보량 ] 등\n",
    "\n",
    "\n",
    "- variance & 상호 정보량이 낮은 feature를 제거하는 것은 유의미한 처리이다.<br>\n",
    "    - 이 경우 모델에 의미있는 정보를 주기보다는 계산에 방해가 될 가능성이 더 크기 때문.<br>\n",
    "    - 이는 wrapper method에서도 하지 않기 때문에 의미가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper method\n",
    "---\n",
    "![그림3](https://user-images.githubusercontent.com/74717033/135283663-6c7ad20a-0156-492d-816f-9f22d7afebc1.png)\n",
    "\n",
    "- 주로 3가지 방식으로 진행한다.\n",
    "    - Foward selection \n",
    "    - backward elimination\n",
    "    - genetic search \n",
    "\n",
    "    \n",
    "- 과적합이 되기 쉽다?\n",
    "    - feature를 잘 골랐다 = 현재 데이터 set에서 가장 잘 맞는 데이터이다.\n",
    "    - 너무 잘 맞는 데이터를 문제 해결을 위한 모델에 적용 -> 과적합 우려\n",
    "    - 따라서, Wrapper method를 한 뒤에는 항상 과적합이 되는지를 확인해봐야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature간의 다중 공선성 확인\n",
    "- feature 수집 이후 selection 이전에 시행해야 한다.\n",
    "---\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/74717033/135288735-f585a156-2d6f-45f8-97eb-ecd553443c73.png)\n",
    "\n",
    "\n",
    "- 독립 변수들 사이에서 높은 상관관계가 나타나는 문제<br><br>\n",
    "    - 회귀 분석의 기본 가정 (독립변수들은 상호 독립적이다) 를 위배<br><br>\n",
    "- 모델의 가중치 (Tree기반 모델은 중요성) 가 분산되어 모델이 올바른 output을 만들지 못하게 한다.\n",
    "<br><br>\n",
    "- feature 사이의 correlation value로 확인가능 하다. (혹은 VIF 로 검정하기도 한다.)<br><br>\n",
    "- 보통 0.9 ~ 0.95 이상의 값이면 상관관계가 높다고 본다.<br><br>\n",
    "- 이때, 양의 상관관계 & 음의 상관관계 모두를 제거해야 한다.<br><br>\n",
    "- 추가로 target과 상관계수가 너무 낮은 feature 또한 제거해 줘야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 중요도 확인하기\n",
    "---\n",
    "\n",
    "- target 결정을 위해 필요한 각 feature의 중요성을 나타내는 지표<br>\n",
    "- 정확도 : Decision tree기반의 모델에서 사용되는 지표<br><br>\n",
    "    - 단순하게 가정할때, decision tree는 (0 , 1) 이라는 두 카테고리 데이터를 분류하는 모델이다.<br><br>\n",
    "    - 이때 이 \"두 데이터를 분류하기 위한 작업에 어떤 feature를 더 많이 잘라냈는가?\" 를 본다면 그것이 feature importance가 된다.<br><br>\n",
    "    - 단, 한번 slice할때 feature 0 또는 feature 1을 각각 한번씩만 자를 수 있다.\n",
    "    \n",
    "![image](https://user-images.githubusercontent.com/74717033/135378854-5f7ad523-93ff-414f-b13d-62951c94c76e.png)\n",
    "\n",
    "- 그림으로 본다면 class 0과 1을 분류하기 위해 feature 1은 2번만 잘라낸 것에 비해 feature 0은 3번 잘라냈다.<br><br>\n",
    "    - 즉, feature 0가 feature 1보다는 더 중요도가 높은 feature이다.\n",
    "\n",
    "- 트리기반 모델에서 feature importance가 갖는 정확도의 정도는 아래와 같다.<br><br>\n",
    "\n",
    "\n",
    "- Decision Tree < Random Forest < XGBoost < Permutation Importance < Impact on SHAP\n",
    "---\n",
    "\n",
    "- 데이터 분석에서는 **중요도와 민감도** 모두 고려해야만 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foward Selection & Backward Elimination (greedy)\n",
    "---\n",
    "- 1) feature를 importance 순으로 sorting한다.<br><br>\n",
    "- 2) sorting된 feature 를 추가 / 제거 하면서 모델의 성능을 반복 측정 -> 최고 정확도를 나타낸 feature set을 선택하는 방법<br><br>\n",
    "    - 0개의 변수에서 시작해 가장 중요도가 높은 feature부터 1개씩 추가해 가면서 모델 정확도 비교 = Foward Selection <br><br>\n",
    "    - 전체 변수에서 가장 중요도가 낮은 feature부터 1개씩 제거하면서 모델 정확도 비교 = Backward Elimination <br><br>\n",
    "- 결국, Forward나 Backward 어느 방식을 사용하든 '최고의 정확도' 를 보이는 feature 조합에서 수렴하게 된다.(그래프 참조)<br><br>\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/74717033/138888905-ba4739d0-0623-4e18-a535-36f0d267d33b.png)<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "- 단, 이 방법을 통해 선택된 feature set은 '최적의 조합'은 될 수 없다.<br><br>\n",
    "- (중요도는 낮아도) 특정 feature간의 조합으로 시너지가 난 경우 등의 사례가 있을 수 있다.<br><br>\n",
    "- 하지만, 실질적으로 현업에서는 이 방법을 가장 많이 사용 한다. (실무적으로 접근이 간편하고, 직관적이기 때문)<br><br>\n",
    "- 단, 특정 연구를 할때 아주 세세한 중요도의 차이일지라도 정말 best of best feature set을 찾고 싶다면 Genetic Search를 활용해야 하는 것이 좋다. (문제는 계산 비용이 매우 많이 발생)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
